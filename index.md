---
layout: titlepage3
---

This class meets Tuesday and Thursday from 10:30 - 11:50 AM in [Gates B03](https://campus-map.stanford.edu/?id=07-450&lat=37.43011014&lng=-122.17341616&zoom=17&srch=Gates).

## Teaching Assistants

<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/Nathan.png">
</div>
<div class="card">
<a class="talkdate" href="https://people.stanford.edu/stanfurd/">Nathan Zhang</a> <br>
<span class="speaker">Office Hours TBA</span> <br>
</div>
</div>

## Class Information
<img src="https://www.nsf.gov/images/logos/NSF_4-Color_bitmap_Logo_thumb.jpg">
<p>
Funding for this research/activity was partially provided by the  National Science Foundation Division of Computing and Communication Foundations under award number <a href="https://nsf.gov/awardsearch/showAward?AWD_ID=1563113&HistoricalAwards=false">1563113</a>.
</p>

## Schedule

<style type="text/css"> .tg { border-collapse:collapse; border-spacing:0; } .tg td { font-family:Arial, sans-serif; font-size:14px; padding:10px 5px; overflow:hidden; word-break:normal; border: 1px solid black; } .tg th { font-family:Arial, sans-serif; font-size:14px; font-weight:normal; padding:10px 5px; overflow:hidden; word-break:normal; } .tg td { font-weight:bold; text-align:center; vertical-align:top; } .tg td:last-child { vertical-align:top; } </style>

<table width="720" class="tg">
<tbody>
<tr>
<td width="36">
<p><strong>Lecture</strong></p>
</td>
<td width="96">
<p><strong>Date</strong></p>
</td>
<td width="294">
<p><strong>Topic</strong></p>
</td>
  
<td width="198">
<p><strong>Reading Assignments Lecture Slides</strong></p>
</td>
<td width="96">
<p><strong>Spatial Assignment</strong></p>
</td>
</tr>
<tr>
<td width="36">
<p>1</p>
</td>
<td width="96">
<p>1/10/2023</p>
</td>
<td width="294">
<p>Introduction,</p>
<p>Software 2.0</p>
<p>Role of hardware accelerators in post Dennard&nbsp;and Moore era</p>
</td>
  
<td width="198">
<p>&nbsp;</p>
</td>
<td width="96">
<p>&nbsp;</p>
</td>
</tr>
<tr>
<td width="36">
<p>2</p>
</td>
<td width="96">
<p>1/12/2023</p>
</td>
<td width="294">
<p>Linear algebra fundamentals and accelerating linear algebra<br> BLAS operations<br></p>
</td>
<td width="198">
<p><u><a href="https://ieeexplore.ieee.org/document/6241647/">Is Dark silicon useful?</a></u><br> Hennessy Patterson Chapter 7.1-7.2</p>
</td>
  
<td width="96">
<p>&nbsp;</p>
</td>
</tr>
<tr>
<td width="36">
<p>3</p>
</td>
<td width="96">
<p>1/17/2023</p>
</td>
<td width="294">
<p>20th century techniques: Systolic arrays and MIMDs, CGRAs RDAs</p>
</td>
<td width="198">
<p><u><a href="http://www.eecs.harvard.edu/~htk/publication/1982-kung-why-systolic-architecture.pdf">Why Systolic Architectures?</a></u><br> <u><a href="https://www.cs.utexas.edu/users/pingali/CS378/2008sp/papers/gotoPaper.pdf">Anatomy of high performance GEMM</a></u></p>
<p><u><a href="https://arxiv.org/abs/1602.04183">Dark Memory</a></u></p>
</td>
<td width="96">
<p>Linear Algebra<br> Accelerators</p>
</td>
</tr>
<tr>
<td width="36">
<p>4</p>
</td>
<td width="96">
<p>1/19/2023</p>
</td>
<td width="294">
<p>Introduction to Spatial: Analyzing Performance and Energy with Spatial</p>
</td>
<td width="198">
<p><u><a href="https://stanford-ppl.github.io/website/papers/pldi18_koeplinger.pdf">Spatial</a></u><br>
<u><a href="https://dl.acm.org/citation.cfm?id=2665689"> Aladdin </a></u>
</p>
<!-- 
<p><u><a href="https://www.cc.gatech.edu/~hadi/doc/paper/2015-tr-tabla.pdf">TABLA</a></u></p>
--> 
<p><u><a href="https://ieeexplore.ieee.org/document/6212466/">Codesign Tradeoffs</a></u></p>
</td>
<td width="96">
<p></p>
</td>
</tr>
<tr>
<td width="36">
<p>5</p>
</td>
<td width="96">
<p>1/24/2023</p>
</td>
<td width="294">
<p>MLPs and CNNs Inference</p>
</td>
<td width="198">

<p><u><a href="http://www.rle.mit.edu/eems/wp-content/uploads/2017/11/2017_pieee_dnn.pdf">Efficient Processing of DNNs</a></u><br></p>
<p><u><a href="http://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf">NVIDIA Tesla V100</a></u></p>

</td>
<td width="96">
<p></p>
</td>
</tr>
<tr>
<td width="36">
<p>6</p>
</td>
<td width="96">
<p>1/26/2023</p>
</td>
<td width="294">
<p>Evaluating Performance, Energy efficiency, Parallelism, Locality,<br> Memory hierarchy, Roofline model</p>
</td>
<td width="198">
<p><u><a href="https://people.eecs.berkeley.edu/~kubitron/cs252/handouts/papers/RooflineVyNoYellow.pdf">Roofline Model </a> </u></p>
<p><u><a href="https://arxiv.org/pdf/1704.04760.pdf">Google TPU</a></u></p>
</td>
<td width="96">
</td>
</tr>


<tr>
<td width="36">
<p>7</p>
</td>
<td width="96">
<p>1/31/2023</p>
</td>
<td width="294">
<p>Fast Implementation of Deep Learning Kernels</p>
<p>&nbsp;</p>
</td>
<td width="198">
  <p><u><a href="https://arxiv.org/abs/1606.04209">Systematic Approach to Blocking</a></u></p>
  <p> <u><a href="https://arxiv.org/pdf/1809.10170.pdf">High Performance Zero-Memory Overhead Direct Convolutions</a></u></p>

</td>
<td width="96">
<p>&nbsp;</p>
</td>
</tr>

<tr>
<td width="36">
<p>8</p>
</td>
<td width="96">
<p>2/2/2023</p>
</td>
<td width="294">
<p>Training 1: SGD, back propagation, statistical efficiency, batch size</p>
</td>
<td width="198">
  <u><a href="https://arxiv.org/pdf/1706.00517.pdf">Caterpillar</a></u><br>
  <u><a href="http://ruder.io/optimizing-gradient-descent/"> Optimizing Gradient Descent </a></u> <br>
</td>
<td width="96">
<p></p>
</td>
</tr>


<tr>
<td width="36">
<p>9</p>
</td>
<td width="96">
<p>2/07/2023</p>
</td>
<td width="294">
<p> Qunatized Deep Learning</p>
</td>
<td width="198">
<p><u><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w33/Gholami_SqueezeNext_Hardware-Aware_Neural_CVPR_2018_paper.pdf">SqueezNext</a></u></p>
</td>
<td width="96">
<p>&nbsp;</p>
</td>
</tr>


<tr>
<td width="36">
<p>10</p>
</td>
<td width="96">
<p>2/9/2023</p>
</td>
<td width="294">
<p> Guest Lecture: David Kanter </p>
<p>GPU Design Tradeoffs for Deeplearning and MLPerf</p>
<p>Nvidia Volta</p>
</td>
<td width="198">
  <u><a href="https://arxiv.org/pdf/1710.03740.pdf">Mixed Precision Training Nvidia</a></u><br>
  <u><a href="https://arxiv.org/abs/1905.12334">Mixed Precision Training With 8-bit Floating Point</a></u>
</td>
<td width="96">
<p></p>
</td>
</tr>

<tr>
<td width="36">
<p>11</p>
</td>
<td width="96">
<p>2/14/2023</p>
</td>
<td width="294">
<p>Sparsity in Deep Learning</p>
</td>
<td width="198">
  <u><a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">NVIDIA H100</a></u> <br>
  <u><a href="https://arxiv.org/pdf/2001.03253.pdf">Campfire</a></u>
</td>
<td width="96">
<p>LeNet Inference</p>
</td>
</tr>


<!--
<tr>

<td width="36">

<p>13</p>

</td>

<td width="96">

<p>2/21/2023</p>

</td>

<p>Language Models</p>

<td width="198">

<a href="https://arxiv.org/abs/1609.08144">GNMT </a> <br>

<a href="https://arxiv.org/abs/1810.04805">BERT </a> 

</td>

<td width="96">

<p>&nbsp;</p>

</td>

</tr>
-->

<tr>
<td width="36">
<p>12</p>
</td>
<td width="96">
<p>2/23/2023</p>
</td>
<td width="294">
<p>Recommendation Systems</p>
</td>
<td width="198">
<a href="https://arxiv.org/pdf/2010.13100.pdf"> Tensor Casting </a> <br>
<a href="https://arxiv.org/pdf/2205.04702.pdf"> Training Personalized Recommendations from Scratch</a>
</td>
<td width="96">
<p>&nbsp;</p>
</td>
</tr>


<tr>
<td width="36">
<p>13</p>
</td>
<td width="96">
<p>2/28/2023</p>
</td>
<td width="294">
<p>Assignment 1 Feedback and Discussion</p>

</td>
<td width="198">

</td>
<td width="96">
<p>&nbsp;</p>
</td>
</tr>

<tr>
<td width="36">
<p>14</p>
</td>
<td width="96">
<p>3/2/2023</p>
</td>
<td width="294">
  <p>Foundational Models,  Graph Neural Networks</p>
</td>
<td width="198">
</td>
<td width="96">
<p> Midterm</p>
</td>
</tr>

<tr>
<td width="36">
<p>15</p>
</td>
<td width="96">
<p>3/7/2023</p>
</td>
<td width="294">
<p>Distributed ML Systems</p>
</td>
<td width="198">

</td>
<td width="96">
<p>&nbsp;</p>
</td>
</tr>


<tr>
<td width="36">
<p>16</p>
</td>
<td width="96">
<p>3/9/2023</p>
</td>
<td width="294">
<p>TBD</p>
</td>
<td width="198">
</td>
<td width="96">
<p>&nbsp;</p>
</td>
</tr>


<tr>
<td width="36">
<p>17</p>
</td>
<td width="96">
<p>3/14/2023</p>
</td>
<td width="294">
<p>Machine Learning Systems and Software Stack</p>
</td>
<td width="198">
<u><a href="https://pdfs.semanticscholar.org/dee4/9d30a19f392ca9d002720a554800fa16d19e.pdf"> Taxonomy of Accelerator Architectures</a></u> <br>
<u><a href="https://dl.acm.org/doi/pdf/10.1145/3317550.3321441?download=true"> ML Systems Stuck in a Rut</a></u> <br>
<p>&nbsp;</p>
</td>
<td width="96">
<p>&nbsp;</p>
</td>
</tr>


<tr>
<td width="36">
<p>19</p>
</td>
<td width="96">
<p>3/16/2023</p>
</td>
<td width="294">
<p>TBD</p>
</td>
<td width="198">
<p>&nbsp;</p>
</td>
<td width="96">
<p>&nbsp;</p>
</td>
</tr>
</tbody>
</table>

## [](#Lectures) Guest Lectures

<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/DavidKanter.png" style="overflow: hidden">
</div>
<div class="card">
<a class="talkdate" href="https://mlcommons.org/en/leadership/">David Kanter, MLCommons</a> <br>
<span class="speaker"> MLPerf</span> <br>
<span class="speakerposition">2/9/2023</span>
</div>
</div>

<!--

<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/kian_katanforoosh.jpg">
</div>
<div class="card">
<a class="talkdate" href="katanforoosh_lecture">Kian Katanforoosh, deeplearning.ai and Stanford University</a> <br>
<span class="speaker">From Machine Learning to Deep Learning: a computational transition</span> <br>
<span class="speakerposition">Thursday January 9, 2020</span>
</div>
</div>



<hr>

<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/Luigi_Nardi.jpg">
</div>
<div class="card">
<a class="talkdate" href="esmaeilzadeh_lecture">Luigi Nardi, Lund University and Stanford University</a> <br>
<span class="speaker"> Design Space Optimization with Spatial</span> <br>
<span class="speakerposition">Thursday January 23, 2020</span>
</div>
</div>

-->

<!--

<hr>

<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/Boris_Ginsburg-200x200.jpg">
</div>
<div class="card">
<a class="talkdate" href="ginsburg_lecture">Boris Ginsburg, NVIDIA</a> <br>
<span class="speaker">Generalization and Regularization of Training</span> <br>
<span class="speakerposition">Tuesday January 28, 2018</span>
</div>
</div>
-->

<!--
<hr>

<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/Azalia.jpg">
</div>
<div class="card">
<a class="talkdate" href="Azalia_lecture">Azalia MirHosseini, Google Brain</a> <br>
<span class="speaker">Reinforcement Learning and Hardware Design</span> <br>
<span class="speakerposition">Thursday January 30, 2020</span>
</div>
</div>

<hr>
<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/Fanny.jpg">
</div>
<div class="card">
<a class="talkdate" href="chung_lecture">Fanny Nina Paravecino, Microsoft Research</a> <br>
<span class="speaker">Real-Time AI at Cloud Scale with Project Brainwave</span> <br>
<span class="speakerposition">Tuesday February 4, 2020</span>
</div>
</div>

<hr>

<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/Amir.jpg">
</div>
<div class="card">
<a class="talkdate" href="ginsburg_lecture">Amir Gholami, UC Berkeley</a> <br>
<span class="speaker">Precision and Quantized Training for Deep Learning</span> <br>
<span class="speakerposition">Thursday February 6, 2020</span>
</div>
</div>

-->

<!--

<hr>
<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/TzeMeng.png">
</div>
<div class="card">
<a class="talkdate" href="ginsburg_lecture">Tze Meng Low, Carnegie Melon University</a> <br>
<span class="speaker">Fast Implementation of Deep Learning Kernels</span> <br>
<span class="speakerposition">Tuesday February 11, 2020</span>
</div>
</div>

-->

<hr>
<!--

<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/paulius-200x200.jpg">
</div>
<div class="card">
<a class="talkdate" href="">Paulius Micikevicius, NVIDIA</a> <br>
<span class="speaker">GPU Design Tradeoffs for Deeplearning and MLPerf</span> <br>
<span class="speakerposition">Thursday February 13, 2020</span>
</div>
</div>
-->



<!--

<hr>
<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/cliff_young.jpg">
</div>
<div class="card">
<a class="talkdate" href="young_lecture">Cliff Young, Google</a> <br>
<span class="speaker">Neural Networks Have Rebooting Computing: What Should We Reboot Next?</span> <br>
<span class="speakerposition">Tuesday February 18, 2020</span>
</div>
</div>


<hr>

<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/Mohammad_Shoeybi.jpeg">
</div>
<div class="card">
<a class="talkdate" href="ginsburg_lecture">Mohammad Shoeybi, NVIDIA</a> <br>
<span class="speaker">Natural Language Processing</span> <br>
<span class="speakerposition">Thursday February 20, 2020</span>
</div>
</div>

<hr>

<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/Mikhail_Smelyanskiy.JPG">
</div>
<div class="card">
<a class="talkdate" href="smelyanskiy_lecture">Mikhail Smelyanskiy, Facebook</a> <br>
<span class="speaker">AI at Facebook Datacenter Scale</span> <br>
<span class="speakerposition">Tuesday February 25, 2020</span>
</div>
</div>

<hr>
<div class="speaker-wrap">
<div class="speakerphoto">
<img src="assets/img/Boris_Ginsburg-200x200.jpg">
</div>
<div class="card">
<a class="talkdate" href="ginsburg_lecture">Boris Ginsburg, NVIDIA</a> <br>
<span class="speaker">Large Batch Training of Convolution Networks</span> <br>
<span class="speakerposition">Tuesday March 3, 2020</span>
</div>
</div>
-->

## [Lecture Notes (Winter 2020)]

## [Lecture Notes (Fall 2018)](https://drive.google.com/drive/folders/1c8m186dCagpB4_sqhaPA19-K8gBlezPk?usp=sharing)

## Related Stanford Courses

-   [CS230](http://cs230.stanford.edu/syllabus.html)
-   [CS231n](http://cs231n.github.io)
-   [STATS 385](https://stats385.github.io/)

## [Reading list and other resources](readings)

## [Basic information about deep learning](basicinfo)

## [Cheat sheet -- things that everyone needs to know](cheat_sheet)

## [Blogs](blogs)

## [Grading](grading)
